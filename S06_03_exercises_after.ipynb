{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a real classifier\n",
    "\n",
    "This code is an example of the use of VADER classifier from NLTK. It is a Naive-Bayes classifier that is trainded with a lexicon and dataset of movie reviews.\n",
    "\n",
    "Look in the example how the library SKLearn is used to evaulate the classifier.\n",
    "\n",
    "At the end you have an example on how to use the classifier en custom examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Classifier Evaluation:\n",
      "Accuracy: 73.00%\n",
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =     15.2 : 1.0\n",
      "                  finest = True              pos : neg    =     13.8 : 1.0\n",
      "                chilling = True              pos : neg    =     12.7 : 1.0\n",
      "               ludicrous = True              neg : pos    =     12.2 : 1.0\n",
      "               affecting = True              pos : neg    =     12.0 : 1.0\n",
      "                  seagal = True              neg : pos    =     11.3 : 1.0\n",
      "                 idiotic = True              neg : pos    =     11.1 : 1.0\n",
      "                 insipid = True              neg : pos    =     10.7 : 1.0\n",
      "                  avoids = True              pos : neg    =     10.6 : 1.0\n",
      "                palpable = True              pos : neg    =     10.0 : 1.0\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.46      0.62       188\n",
      "         pos       0.67      0.97      0.79       212\n",
      "\n",
      "    accuracy                           0.73       400\n",
      "   macro avg       0.80      0.71      0.70       400\n",
      "weighted avg       0.79      0.73      0.71       400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 87 101]\n",
      " [  7 205]]\n",
      "\n",
      "VADER Sentiment Analysis:\n",
      "Sentence: I absolutely loved this movie! The acting was fantastic.\n",
      "Sentiment: positive (Score: 0.8436)\n",
      "\n",
      "Sentence: This was the worst film I have ever seen.\n",
      "Sentiment: negative (Score: -0.6249)\n",
      "\n",
      "Sentence: The plot was predictable, but the cinematography was beautiful.\n",
      "Sentiment: positive (Score: 0.7469)\n",
      "\n",
      "Sentence: I wouldn't recommend it. It was boring and too long.\n",
      "Sentiment: negative (Score: -0.5283)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "\n",
    "# Download required NLTK datasets\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Preprocess the data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_features(words):\n",
    "    return {word: True for word in words if word.lower() not in stop_words}\n",
    "\n",
    "# Prepare the dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)  # Shuffle the dataset for better randomness\n",
    "\n",
    "# Feature extraction\n",
    "feature_sets = [(extract_features(words), category) for (words, category) in documents]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(feature_sets) * 0.8)\n",
    "train_set, test_set = feature_sets[:train_size], feature_sets[train_size:]\n",
    "\n",
    "# Train a Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"\\nNaive Bayes Classifier Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy(classifier, test_set) * 100:.2f}%\")\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "# Prepare predictions and true labels for sklearn metrics\n",
    "y_true = [label for (_, label) in test_set]\n",
    "y_pred = [classifier.classify(features) for (features, _) in test_set]\n",
    "\n",
    "# Evaluate using sklearn metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# VADER Sentiment Analysis on custom examples\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "example_sentences = [\n",
    "    \"I absolutely loved this movie! The acting was fantastic.\",\n",
    "    \"This was the worst film I have ever seen.\",\n",
    "    \"The plot was predictable, but the cinematography was beautiful.\",\n",
    "    \"I wouldn't recommend it. It was boring and too long.\"\n",
    "]\n",
    "\n",
    "print(\"\\nVADER Sentiment Analysis:\")\n",
    "for sentence in example_sentences:\n",
    "    score = sia.polarity_scores(sentence)\n",
    "    sentiment = \"positive\" if score['compound'] > 0 else \"negative\"\n",
    "    print(f\"Sentence: {sentence}\\nSentiment: {sentiment} (Score: {score['compound']})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "Create your own gold standard and measure Precission, Recall, and F1 manually and with SKLearn to check if the result is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive']\n",
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "gold_standard = [\n",
    "    (\"Today is a great day.\", \"positive\"),\n",
    "    (\"I don't like this movie.\", \"negative\"),\n",
    "    (\"This was the best day ever.\", \"positive\"),\n",
    "    (\"He is behaving worse than ever.\", \"negative\"),\n",
    "    (\"This was an amazing story, where did you learn it?\", \"positive\"),\n",
    "    (\"He is the worst person I know.\", \"negative\"),\n",
    "    (\"Have a good day.\", \"positive\"),\n",
    "    (\"What a bad day, it's raining.\", \"negative\"),\n",
    "    (\"I love the rain.\", \"positive\"),\n",
    "    (\"Not worth your time.\", \"negative\")\n",
    "]\n",
    "\n",
    "feature_sets = [(extract_features(word_tokenize(sentence)), label) for (sentence, label) in gold_standard]\n",
    "\n",
    "test_set = feature_sets\n",
    "\n",
    "y_true = [label for (_, label) in test_set]\n",
    "y_pred = [classifier.classify(features) for (features, _) in test_set]\n",
    "y_pred = [\"positive\" if label == \"pos\" else \"negative\" for label in y_pred]\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "\n",
    "#Manually\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "true_negatives= 0\n",
    "\n",
    "for true_label, pred_label in zip(y_true, y_pred):\n",
    "    if true_label == \"positive\" and pred_label == \"positive\":\n",
    "        true_positives += 1\n",
    "    elif true_label == \"negative\" and pred_label == \"positive\":\n",
    "        false_positives += 1\n",
    "    elif true_label == \"positive\" and pred_label == \"negative\":\n",
    "        false_negatives += 1\n",
    "    elif true_label == \"negative\" and pred_label == \"negative\":\n",
    "        true_negatives += 1\n",
    "print(true_positives)\n",
    "print(false_negatives)\n",
    "\n",
    "\n",
    "#precission = true_positives / (true_positives + false_positives)\n",
    "\n",
    "#recall = true_positives / (true_positives + false_negatives)\n",
    "#\n",
    "#f1 = 2 * (precission * recall) / (precission + recall)\n",
    "#\n",
    "#print(f\"Precission manually: {precission:.2f}\")\n",
    "#print(f\"Recall manually: {recall:.2f}\")\n",
    "#print(f\"F1 manually: {f1:.2f}\")\n",
    "#\n",
    "##Scikit-learn\n",
    "#precission = precision_score(y_true, y_pred, pos_label=\"positive\")\n",
    "#recall = recall_score(y_true, y_pred, pos_label=\"positive\")\n",
    "#f1 = f1_score(y_true, y_pred, pos_label=\"positive\")\n",
    "#\n",
    "#print(f\"Precission sccikit: {precission:.2f}\")\n",
    "#print(f\"Recall scikit: {recall:.2f}\")\n",
    "#print(f\"F1 scikit: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
