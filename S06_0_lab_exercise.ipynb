{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the next text, perform the following actions\n",
    "text = \"The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old. Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.\"\n",
    "\n",
    "# (1 point) 1 - Use NLTK to split the sentences \n",
    "\n",
    "# (2 points) 2 - Convert with regex the acronym U.S.A. to USA, the number 1.9m to 190 centimeters or any other number of a height like that (e.g. 1.75m to 175 centimeters), and \"$5.5 billion\" to five point five billion. \n",
    "\n",
    "# (1 point) 3 - Convert to lowercase except the proper nouns that must keep the original case. For the multiword proper names convert them to an unique word joining the two word with underscoere (Juan Fernández -> Juan_Fernández).\n",
    "\n",
    "# (1 point) 4 - Tokenize the text (use the tool you prefer). \n",
    "\n",
    "# (1 point) 5 - Remove the stopwords (use the tool you prefer). \n",
    "\n",
    "# (1 point) 6 - Create bigrams with pure python.\n",
    "\n",
    "# (2 point) 7 - Create a language model that predict the next word using bigrams. Please explain in the code how you made the calculations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The president of the U.S.A., Donald Trump, is 1.9m high and 78 years old.', 'Forbes Magazine has assessed his wealth, currently estimating it at $5.5 billion as of mid-February 2025.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manue\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "splitsentence= sent_tokenize(text)\n",
    "\n",
    "print(splitsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alltext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     right_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(num_to_words[r] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m right)\n\u001b[0;32m     23\u001b[0m     alltext \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m billion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m point \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mright_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m billion\u001b[39m\u001b[38;5;124m'\u001b[39m, ntext)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(alltext)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alltext' is not defined"
     ]
    }
   ],
   "source": [
    "#2\n",
    "patern= r'\\bU\\.S\\.A\\b'\n",
    "npatern= r'(\\d+)\\.(\\d+)m'\n",
    "mpatern= r'(\\d+)\\.(\\d+)billion'\n",
    "sub= re.sub(patern, 'USA', text)\n",
    "numbers= re.findall(npatern, text)\n",
    "moneys= re.findall(mpatern, text)\n",
    "num_to_words = {\n",
    "    '0': 'zero', '1': 'one', '2': 'two', '3': 'three', '4': 'four',\n",
    "    '5': 'five', '6': 'six', '7': 'seven', '8': 'eight', '9': 'nine'\n",
    "}\n",
    "for number in numbers:\n",
    "    meters = int(number[0])\n",
    "    decimeters = int(number[1])\n",
    "    total_cm = meters * 100  + decimeters*10\n",
    "    ntext = re.sub(f'{meters}\\.{decimeters}m', f'{total_cm} centimeters', sub)\n",
    "    \n",
    "for money in moneys:\n",
    "    left = money[0]\n",
    "    right = money[1]\n",
    "    left_word = ' '.join(num_to_words[l] for l in left)\n",
    "    right_word = ' '.join(num_to_words[r] for r in right)\n",
    "    alltext = re.sub(f'\\${left}\\.{right} billion', f'{left_word} point {right_word} billion', ntext)\n",
    "\n",
    "print(alltext)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
